{
    "page_content": "# Human-in-the-loop\u00b6\n\nThis guide uses the new `interrupt` function.\n\nAs of LangGraph 0.2.57, the recommended way to set breakpoints is using the `interrupt` function as it simplifies **human-in-the-loop** patterns.\n\nIf you're looking for the previous version of this conceptual guide, which relied on static breakpoints and `NodeInterrupt` exception, it is available here. \n\nA **human-in-the-loop** (or \"on-the-loop\") workflow integrates human input into automated processes, allowing for decisions, validation, or corrections at key stages. This is especially useful in **LLM-based applications** , where the underlying model may generate occasional inaccuracies. In low-error-tolerance scenarios like compliance, decision-making, or content generation, human involvement ensures reliability by enabling review, correction, or override of model outputs.\n\n## Use cases\u00b6\n\nKey use cases for **human-in-the-loop** workflows in LLM-based applications include:\n\n  1. **\ud83d\udee0\ufe0f Reviewing tool calls**: Humans can review, edit, or approve tool calls requested by the LLM before tool execution.\n  2. **\u2705 Validating LLM outputs** : Humans can review, edit, or approve content generated by the LLM.\n  3. **\ud83d\udca1 Providing context** : Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.\n\n\n\n## `interrupt`\u00b6\n\nThe `interrupt` function in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, presenting information to a human, and resuming the graph with their input. This function is useful for tasks like approvals, edits, or collecting additional input. The `interrupt` function is used in conjunction with the `Command` object to resume the graph with a value provided by the human.\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: State):\n        value = interrupt(\n            # Any JSON serializable value to surface to the human.\n            # For example, a question or a piece of text or a set of keys in the state\n           {\n              \"text_to_revise\": state[\"some_text\"]\n           }\n        )\n        # Update the state with the human's input or route the graph based on the input.\n        return {\n            \"some_text\": value\n        }\n    \n    graph = graph_builder.compile(\n        checkpointer=checkpointer # Required for `interrupt` to work\n    )\n    \n    # Run the graph until the interrupt\n    thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n    graph.invoke(some_input, config=thread_config)\n    \n    # Resume the graph with the human's input\n    graph.invoke(Command(resume=value_from_human), config=thread_config)\n    \n[/code]\n\nAPI Reference: interrupt\n[code] \n    {'some_text': 'Edited text'}\n    \n[/code]\n\nWarning\n\nInterrupts are both powerful and ergonomic. However, while they may resemble Python's input() function in terms of developer experience, it's important to note that they do not automatically resume execution from the interruption point. Instead, they rerun the entire node where the interrupt was used. For this reason, interrupts are typically best placed at the start of a node or in a dedicated node. Please read the resuming from an interrupt section for more details. \n\nFull Code\n\nHere's a full example of how to use `interrupt` in a graph, if you'd like to see the code in action.\n[code] \n    from typing import TypedDict\n    import uuid\n    \n    from langgraph.checkpoint.memory import MemorySaver\n    from langgraph.constants import START\n    from langgraph.graph import StateGraph\n    from langgraph.types import interrupt, Command\n    \n    class State(TypedDict):\n       \"\"\"The graph state.\"\"\"\n       some_text: str\n    \n    def human_node(state: State):\n       value = interrupt(\n          # Any JSON serializable value to surface to the human.\n          # For example, a question or a piece of text or a set of keys in the state\n          {\n             \"text_to_revise\": state[\"some_text\"]\n          }\n       )\n       return {\n          # Update the state with the human's input\n          \"some_text\": value\n       }\n    \n    \n    # Build the graph\n    graph_builder = StateGraph(State)\n    # Add the human-node to the graph\n    graph_builder.add_node(\"human_node\", human_node)\n    graph_builder.add_edge(START, \"human_node\")\n    \n    # A checkpointer is required for `interrupt` to work.\n    checkpointer = MemorySaver()\n    graph = graph_builder.compile(\n       checkpointer=checkpointer\n    )\n    \n    # Pass a thread ID to the graph to run it.\n    thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n    \n    # Using stream() to directly surface the `__interrupt__` information.\n    for chunk in graph.stream({\"some_text\": \"Original text\"}, config=thread_config):\n       print(chunk)\n    \n    # Resume using Command\n    for chunk in graph.stream(Command(resume=\"Edited text\"), config=thread_config):\n       print(chunk)\n    \n[/code]\n\nAPI Reference: MemorySaver | START | StateGraph | interrupt | Command\n[code] \n    {'__interrupt__': (\n          Interrupt(\n             value={'question': 'Please revise the text', 'some_text': 'Original text'}, \n             resumable=True, \n             ns=['human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6'], \n             when='during'\n          ),\n       )\n    }\n    {'human_node': {'some_text': 'Edited text'}}\n    \n[/code]\n\n## Requirements\u00b6\n\nTo use `interrupt` in your graph, you need to:\n\n  1. **Specify a checkpointer** to save the graph state after each step.\n  2. **Call`interrupt()`** in the appropriate place. See the Design Patterns section for examples.\n  3. **Run the graph** with a **thread ID** until the `interrupt` is hit.\n  4. **Resume execution** using `invoke`/`ainvoke`/`stream`/`astream` (see **The`Command` primitive**).\n\n\n\n## Design Patterns\u00b6\n\nThere are typically three different **actions** that you can do with a human-in-the-loop workflow:\n\n  1. **Approve or Reject** : Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involve **routing** the graph based on the human's input.\n  2. **Edit Graph State** : Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves **updating** the state with the human's input.\n  3. **Get Input** : Explicitly request human input at a particular step in the graph. This is useful for collecting additional information or context to inform the agent's decision-making process or for supporting **multi-turn conversations**.\n\n\n\nBelow we show different design patterns that can be implemented using these **actions**.\n\n### Approve or Reject\u00b6\n\nDepending on the human's approval or rejection, the graph can proceed with the action or take an alternative path.\n\nPause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action.\n[code] \n    from typing import Literal\n    from langgraph.types import interrupt, Command\n    \n    def human_approval(state: State) -> Command[Literal[\"some_node\", \"another_node\"]]:\n        is_approved = interrupt(\n            {\n                \"question\": \"Is this correct?\",\n                # Surface the output that should be\n                # reviewed and approved by the human.\n                \"llm_output\": state[\"llm_output\"]\n            }\n        )\n    \n        if is_approved:\n            return Command(goto=\"some_node\")\n        else:\n            return Command(goto=\"another_node\")\n    \n    # Add the node to the graph in an appropriate location\n    # and connect it to the relevant nodes.\n    graph_builder.add_node(\"human_approval\", human_approval)\n    graph = graph_builder.compile(checkpointer=checkpointer)\n    \n    # After running the graph and hitting the interrupt, the graph will pause.\n    # Resume it with either an approval or rejection.\n    thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n    graph.invoke(Command(resume=True), config=thread_config)\n    \n[/code]\n\nAPI Reference: interrupt | Command\n\nSee how to review tool calls for a more detailed example.\n\n### Review & Edit State\u00b6\n\nA human can review and edit the state of the graph. This is useful for correcting mistakes or updating the state with additional information. \n[code] \n    from langgraph.types import interrupt\n    \n    def human_editing(state: State):\n        ...\n        result = interrupt(\n            # Interrupt information to surface to the client.\n            # Can be any JSON serializable value.\n            {\n                \"task\": \"Review the output from the LLM and make any necessary edits.\",\n                \"llm_generated_summary\": state[\"llm_generated_summary\"]\n            }\n        )\n    \n        # Update the state with the edited text\n        return {\n            \"llm_generated_summary\": result[\"edited_text\"] \n        }\n    \n    # Add the node to the graph in an appropriate location\n    # and connect it to the relevant nodes.\n    graph_builder.add_node(\"human_editing\", human_editing)\n    graph = graph_builder.compile(checkpointer=checkpointer)\n    \n    ...\n    \n    # After running the graph and hitting the interrupt, the graph will pause.\n    # Resume it with the edited text.\n    thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n    graph.invoke(\n        Command(resume={\"edited_text\": \"The edited text\"}), \n        config=thread_config\n    )\n    \n[/code]\n\nAPI Reference: interrupt\n\nSee How to wait for user input using interrupt for a more detailed example.\n\n### Review Tool Calls\u00b6\n\nA human can review and edit the output from the LLM before proceeding. This is particularly critical in applications where the tool calls requested by the LLM may be sensitive or require human oversight. \n[code] \n    def human_review_node(state) -> Command[Literal[\"call_llm\", \"run_tool\"]]:\n        # This is the value we'll be providing via Command(resume=<human_review>)\n        human_review = interrupt(\n            {\n                \"question\": \"Is this correct?\",\n                # Surface tool calls for review\n                \"tool_call\": tool_call\n            }\n        )\n    \n        review_action, review_data = human_review\n    \n        # Approve the tool call and continue\n        if review_action == \"continue\":\n            return Command(goto=\"run_tool\")\n    \n        # Modify the tool call manually and then continue\n        elif review_action == \"update\":\n            ...\n            updated_msg = get_updated_msg(review_data)\n            # Remember that to modify an existing message you will need\n            # to pass the message with a matching ID.\n            return Command(goto=\"run_tool\", update={\"messages\": [updated_message]})\n    \n        # Give natural language feedback, and then pass that back to the agent\n        elif review_action == \"feedback\":\n            ...\n            feedback_msg = get_feedback_msg(review_data)\n            return Command(goto=\"call_llm\", update={\"messages\": [feedback_msg]})\n    \n[/code]\n\nSee how to review tool calls for a more detailed example.\n\n### Multi-turn conversation\u00b6\n\nA **multi-turn conversation** architecture where an **agent** and **human node** cycle back and forth until the agent decides to hand off the conversation to another agent or another part of the system. \n\nA **multi-turn conversation** involves multiple back-and-forth interactions between an agent and a human, which can allow the agent to gather additional information from the human in a conversational manner.\n\nThis design pattern is useful in an LLM application consisting of multiple agents. One or more agents may need to carry out multi-turn conversations with a human, where the human provides input or feedback at different stages of the conversation. For simplicity, the agent implementation below is illustrated as a single node, but in reality it may be part of a larger graph consisting of multiple nodes and include a conditional edge.\n\nUsing a human node per agentSharing human node across multiple agents\n\nIn this pattern, each agent has its own human node for collecting user input. This can be achieved by either naming the human nodes with unique names (e.g., \"human for agent 1\", \"human for agent 2\") or by using subgraphs where a subgraph contains a human node and an agent node.\n[code] \n    from langgraph.types import interrupt\n    \n    def human_input(state: State):\n        human_message = interrupt(\"human_input\")\n        return {\n            \"messages\": [\n                {\n                    \"role\": \"human\",\n                    \"content\": human_message\n                }\n            ]\n        }\n    \n    def agent(state: State):\n        # Agent logic\n        ...\n    \n    graph_builder.add_node(\"human_input\", human_input)\n    graph_builder.add_edge(\"human_input\", \"agent\")\n    graph = graph_builder.compile(checkpointer=checkpointer)\n    \n    # After running the graph and hitting the interrupt, the graph will pause.\n    # Resume it with the human's input.\n    graph.invoke(\n        Command(resume=\"hello!\"),\n        config=thread_config\n    )\n    \n[/code]\n\nAPI Reference: interrupt\n\nIn this pattern, a single human node is used to collect user input for multiple agents. The active agent is determined from the state, so after human input is collected, the graph can route to the correct agent.\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", ...]]:\n        \"\"\"A node for collecting user input.\"\"\"\n        user_input = interrupt(value=\"Ready for user input.\")\n    \n        # Determine the **active agent** from the state, so \n        # we can route to the correct agent after collecting input.\n        # For example, add a field to the state or use the last active agent.\n        # or fill in `name` attribute of AI messages generated by the agents.\n        active_agent = ... \n    \n        return Command(\n            update={\n                \"messages\": [{\n                    \"role\": \"human\",\n                    \"content\": user_input,\n                }]\n            },\n            goto=active_agent,\n        )\n    \n[/code]\n\nAPI Reference: interrupt\n\nSee how to implement multi-turn conversations for a more detailed example.\n\n### Validating human input\u00b6\n\nIf you need to validate the input provided by the human within the graph itself (rather than on the client side), you can achieve this by using multiple interrupt calls within a single node.\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: State):\n        \"\"\"Human node with validation.\"\"\"\n        question = \"What is your age?\"\n    \n        while True:\n            answer = interrupt(question)\n    \n            # Validate answer, if the answer isn't valid ask for input again.\n            if not isinstance(answer, int) or answer < 0:\n                question = f\"'{answer} is not a valid age. What is your age?\"\n                answer = None\n                continue\n            else:\n                # If the answer is valid, we can proceed.\n                break\n    \n        print(f\"The human in the loop is {answer} years old.\")\n        return {\n            \"age\": answer\n        }\n    \n[/code]\n\nAPI Reference: interrupt\n\n## The `Command` primitive\u00b6\n\nWhen using the `interrupt` function, the graph will pause at the interrupt and wait for user input.\n\nGraph execution can be resumed using the Command primitive which can be passed through the `invoke`, `ainvoke`, `stream` or `astream` methods.\n\nThe `Command` primitive provides several options to control and modify the graph's state during resumption:\n\n  1. **Pass a value to the`interrupt`**: Provide data, such as a user's response, to the graph using `Command(resume=value)`. Execution resumes from the beginning of the node where the `interrupt` was used, however, this time the `interrupt(...)` call will return the value passed in the `Command(resume=value)` instead of pausing the graph.\n[code]     # Resume graph execution with the user's input.\n    graph.invoke(Command(resume={\"age\": \"25\"}), thread_config)\n    \n[/code]\n\n  2. **Update the graph state** : Modify the graph state using `Command(update=update)`. Note that resumption starts from the beginning of the node where the `interrupt` was used. Execution resumes from the beginning of the node where the `interrupt` was used, but with the updated state.\n[code]     # Update the graph state and resume.\n    # You must provide a `resume` value if using an `interrupt`.\n    graph.invoke(Command(update={\"foo\": \"bar\"}, resume=\"Let's go!!!\"), thread_config)\n    \n[/code]\n\n\n\n\nBy leveraging `Command`, you can resume graph execution, handle user inputs, and dynamically adjust the graph's state.\n\n## Using with `invoke` and `ainvoke`\u00b6\n\nWhen you use `stream` or `astream` to run the graph, you will receive an `Interrupt` event that let you know the `interrupt` was triggered. \n\n`invoke` and `ainvoke` do not return the interrupt information. To access this information, you must use the get_state method to retrieve the graph state after calling `invoke` or `ainvoke`.\n[code] \n    # Run the graph up to the interrupt \n    result = graph.invoke(inputs, thread_config)\n    # Get the graph state to get interrupt information.\n    state = graph.get_state(thread_config)\n    # Print the state values\n    print(state.values)\n    # Print the pending tasks\n    print(state.tasks)\n    # Resume the graph with the user's input.\n    graph.invoke(Command(resume={\"age\": \"25\"}), thread_config)\n    \n[/code]\n[code] \n    {'foo': 'bar'} # State values\n    (\n        PregelTask(\n            id='5d8ffc92-8011-0c9b-8b59-9d3545b7e553', \n            name='node_foo', \n            path=('__pregel_pull', 'node_foo'), \n            error=None, \n            interrupts=(Interrupt(value='value_in_interrupt', resumable=True, ns=['node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553'], when='during'),), state=None, \n            result=None\n        ),\n    ) # Pending tasks. interrupts \n    \n[/code]\n\n## How does resuming from an interrupt work?\u00b6\n\nWarning\n\nResuming from an `interrupt` is **different** from Python's `input()` function, where execution resumes from the exact point where the `input()` function was called.\n\nA critical aspect of using `interrupt` is understanding how resuming works. When you resume execution after an `interrupt`, graph execution starts from the **beginning** of the **graph node** where the last `interrupt` was triggered.\n\n**All** code from the beginning of the node to the `interrupt` will be re-executed.\n[code] \n    counter = 0\n    def node(state: State):\n        # All the code from the beginning of the node to the interrupt will be re-executed\n        # when the graph resumes.\n        global counter\n        counter += 1\n        print(f\"> Entered the node: {counter} # of times\")\n        # Pause the graph and wait for user input.\n        answer = interrupt()\n        print(\"The value of counter is:\", counter)\n        ...\n    \n[/code]\n\nUpon **resuming** the graph, the counter will be incremented a second time, resulting in the following output:\n[code] \n    > Entered the node: 2 # of times\n    The value of counter is: 2\n    \n[/code]\n\n## Common Pitfalls\u00b6\n\n### Side-effects\u00b6\n\nPlace code with side effects, such as API calls, **after** the `interrupt` to avoid duplication, as these are re-triggered every time the node is resumed. \n\nSide effects before interrupt (BAD)Side effects after interrupt (OK)Side effects in a separate node (OK)\n\nThis code will re-execute the API call another time when the node is resumed from the `interrupt`.\n\nThis can be problematic if the API call is not idempotent or is just expensive.\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: State):\n        \"\"\"Human node with validation.\"\"\"\n        api_call(...) # This code will be re-executed when the node is resumed.\n        answer = interrupt(question)\n    \n[/code]\n\nAPI Reference: interrupt\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: State):\n        \"\"\"Human node with validation.\"\"\"\n    \n        answer = interrupt(question)\n    \n        api_call(answer) # OK as it's after the interrupt\n    \n[/code]\n\nAPI Reference: interrupt\n[code] \n    from langgraph.types import interrupt\n    \n    def human_node(state: State):\n        \"\"\"Human node with validation.\"\"\"\n    \n        answer = interrupt(question)\n    \n        return {\n            \"answer\": answer\n        }\n    \n    def api_call_node(state: State):\n        api_call(...) # OK as it's in a separate node\n    \n[/code]\n\nAPI Reference: interrupt\n\n### Subgraphs called as functions\u00b6\n\nWhen invoking a subgraph as a function, the **parent graph** will resume execution from the **beginning of the node** where the subgraph was invoked (and where an `interrupt` was triggered). Similarly, the **subgraph** , will resume from the **beginning of the node** where the `interrupt()` function was called.\n\nFor example,\n[code] \n    def node_in_parent_graph(state: State):\n        some_code()  # <-- This will re-execute when the subgraph is resumed.\n        # Invoke a subgraph as a function.\n        # The subgraph contains an `interrupt` call.\n        subgraph_result = subgraph.invoke(some_input)\n        ...\n    \n[/code]\n\n**Example: Parent and Subgraph Execution Flow**\n\nSay we have a parent graph with 3 nodes:\n\n**Parent Graph** : `node_1` \u2192 `node_2` (subgraph call) \u2192 `node_3`\n\nAnd the subgraph has 3 nodes, where the second node contains an `interrupt`:\n\n**Subgraph** : `sub_node_1` \u2192 `sub_node_2` (`interrupt`) \u2192 `sub_node_3`\n\nWhen resuming the graph, the execution will proceed as follows:\n\n  1. **Skip`node_1`** in the parent graph (already executed, graph state was saved in snapshot).\n  2. **Re-execute`node_2`** in the parent graph from the start.\n  3. **Skip`sub_node_1`** in the subgraph (already executed, graph state was saved in snapshot).\n  4. **Re-execute`sub_node_2`** in the subgraph from the beginning.\n  5. Continue with `sub_node_3` and subsequent nodes.\n\n\n\nHere is abbreviated example code that you can use to understand how subgraphs work with interrupts. It counts the number of times each node is entered and prints the count.\n[code] \n    import uuid\n    from typing import TypedDict\n    \n    from langgraph.graph import StateGraph\n    from langgraph.constants import START\n    from langgraph.types import interrupt, Command\n    from langgraph.checkpoint.memory import MemorySaver\n    \n    \n    class State(TypedDict):\n       \"\"\"The graph state.\"\"\"\n       state_counter: int\n    \n    \n    counter_node_in_subgraph = 0\n    \n    def node_in_subgraph(state: State):\n       \"\"\"A node in the sub-graph.\"\"\"\n       global counter_node_in_subgraph\n       counter_node_in_subgraph += 1  # This code will **NOT** run again!\n       print(f\"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times\")\n    \n    counter_human_node = 0\n    \n    def human_node(state: State):\n       global counter_human_node\n       counter_human_node += 1 # This code will run again!\n       print(f\"Entered human_node in sub-graph a total of {counter_human_node} times\")\n       answer = interrupt(\"what is your name?\")\n       print(f\"Got an answer of {answer}\")\n    \n    \n    checkpointer = MemorySaver()\n    \n    subgraph_builder = StateGraph(State)\n    subgraph_builder.add_node(\"some_node\", node_in_subgraph)\n    subgraph_builder.add_node(\"human_node\", human_node)\n    subgraph_builder.add_edge(START, \"some_node\")\n    subgraph_builder.add_edge(\"some_node\", \"human_node\")\n    subgraph = subgraph_builder.compile(checkpointer=checkpointer)\n    \n    \n    counter_parent_node = 0\n    \n    def parent_node(state: State):\n       \"\"\"This parent node will invoke the subgraph.\"\"\"\n       global counter_parent_node\n    \n       counter_parent_node += 1 # This code will run again on resuming!\n       print(f\"Entered `parent_node` a total of {counter_parent_node} times\")\n    \n       # Please note that we're intentionally incrementing the state counter\n       # in the graph state as well to demonstrate that the subgraph update\n       # of the same key will not conflict with the parent graph (until\n       subgraph_state = subgraph.invoke(state)\n       return subgraph_state\n    \n    \n    builder = StateGraph(State)\n    builder.add_node(\"parent_node\", parent_node)\n    builder.add_edge(START, \"parent_node\")\n    \n    # A checkpointer must be enabled for interrupts to work!\n    checkpointer = MemorySaver()\n    graph = builder.compile(checkpointer=checkpointer)\n    \n    config = {\n       \"configurable\": {\n          \"thread_id\": uuid.uuid4(),\n       }\n    }\n    \n    for chunk in graph.stream({\"state_counter\": 1}, config):\n       print(chunk)\n    \n    print('--- Resuming ---')\n    \n    for chunk in graph.stream(Command(resume=\"35\"), config):\n       print(chunk)\n    \n[/code]\n\nAPI Reference: StateGraph | START | interrupt | Command | MemorySaver\n\nThis will print out\n[code] \n    --- First invocation ---\n    In parent node: {'foo': 'bar'}\n    Entered `parent_node` a total of 1 times\n    Entered `node_in_subgraph` a total of 1 times\n    Entered human_node in sub-graph a total of 1 times\n    {'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b', 'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c'], when='during'),)}\n    \n    --- Resuming ---\n    In parent node: {'foo': 'bar'}\n    Entered `parent_node` a total of 2 times\n    Entered human_node in sub-graph a total of 2 times\n    Got an answer of 35\n    {'parent_node': None} \n    \n[/code]\n\n### Using multiple interrupts\u00b6\n\nUsing multiple interrupts within a **single** node can be helpful for patterns like validating human input. However, using multiple interrupts in the same node can lead to unexpected behavior if not handled carefully.\n\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is **strictly index-based** , so the order of interrupt calls within the node is critical.\n\nTo avoid issues, refrain from dynamically changing the node's structure between executions. This includes adding, removing, or reordering interrupt calls, as such changes can result in mismatched indices. These problems often arise from unconventional patterns, such as mutating state via `Command(resume=..., update=SOME_STATE_MUTATION)` or relying on global variables to modify the node\u2019s structure dynamically.\n\nExample of incorrect code\n[code] \n    import uuid\n    from typing import TypedDict, Optional\n    \n    from langgraph.graph import StateGraph\n    from langgraph.constants import START \n    from langgraph.types import interrupt, Command\n    from langgraph.checkpoint.memory import MemorySaver\n    \n    \n    class State(TypedDict):\n        \"\"\"The graph state.\"\"\"\n    \n        age: Optional[str]\n        name: Optional[str]\n    \n    \n    def human_node(state: State):\n        if not state.get('name'):\n            name = interrupt(\"what is your name?\")\n        else:\n            name = \"N/A\"\n    \n        if not state.get('age'):\n            age = interrupt(\"what is your age?\")\n        else:\n            age = \"N/A\"\n    \n        print(f\"Name: {name}. Age: {age}\")\n    \n        return {\n            \"age\": age,\n            \"name\": name,\n        }\n    \n    \n    builder = StateGraph(State)\n    builder.add_node(\"human_node\", human_node)\n    builder.add_edge(START, \"human_node\")\n    \n    # A checkpointer must be enabled for interrupts to work!\n    checkpointer = MemorySaver()\n    graph = builder.compile(checkpointer=checkpointer)\n    \n    config = {\n        \"configurable\": {\n            \"thread_id\": uuid.uuid4(),\n        }\n    }\n    \n    for chunk in graph.stream({\"age\": None, \"name\": None}, config):\n        print(chunk)\n    \n    for chunk in graph.stream(Command(resume=\"John\", update={\"name\": \"foo\"}), config):\n        print(chunk)\n    \n[/code]\n\nAPI Reference: StateGraph | START | interrupt | Command | MemorySaver\n[code] \n    {'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba'], when='during'),)}\n    Name: N/A. Age: John\n    {'human_node': {'age': 'John', 'name': 'N/A'}}\n    \n[/code]\n\n## Additional Resources \ud83d\udcda\u00b6\n\n  * **Conceptual Guide: Persistence**: Read the persistence guide for more context on replaying.\n  * **How to Guides: Human-in-the-loop**: Learn how to implement human-in-the-loop workflows in LangGraph.\n  * **How to implement multi-turn conversations**: Learn how to implement multi-turn conversations in LangGraph.\n\n\n\n## Comments\n",
    "metadata": {
        "url": "https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/",
        "title": "Human-in-the-loop",
        "description": "Build language agents as graphs",
        "keywords": "No keywords"
    }
}