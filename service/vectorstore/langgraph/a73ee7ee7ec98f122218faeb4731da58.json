{
    "page_content": "# Types\u00b6\n\n##  `` `All = Literal['*']` `module-attribute` \u00b6\n\nSpecial value to indicate that graph should interrupt on all nodes.\n\n##  `` `StreamMode = Literal['values', 'updates', 'debug', 'messages', 'custom']` `module-attribute` \u00b6\n\nHow the stream method should emit outputs.\n\n  * `\"values\"`: Emit all values in the state after each step. When used with functional API, values are emitted once at the end of the workflow.\n  * `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n  * `\"custom\"`: Emit custom data using from inside nodes or tasks using `StreamWriter`.\n  * `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n  * `\"debug\"`: Emit debug events with as much information as possible for each step.\n\n\n\n##  `` `StreamWriter = Callable[[Any], None]` `module-attribute` \u00b6\n\nCallable that accepts a single argument and writes it to the output stream. Always injected into nodes if requested as a keyword argument, but it's a no-op when not using stream_mode=\"custom\".\n\n##  `` `RetryPolicy` \u00b6\n\nBases: `NamedTuple`\n\nConfiguration for retrying nodes.\n\n###  `` `initial_interval: float = 0.5` `class-attribute` `instance-attribute` \u00b6\n\nAmount of time that must elapse before the first retry occurs. In seconds.\n\n###  `` `backoff_factor: float = 2.0` `class-attribute` `instance-attribute` \u00b6\n\nMultiplier by which the interval increases after each retry.\n\n###  `` `max_interval: float = 128.0` `class-attribute` `instance-attribute` \u00b6\n\nMaximum amount of time that may elapse between retries. In seconds.\n\n###  `` `max_attempts: int = 3` `class-attribute` `instance-attribute` \u00b6\n\nMaximum number of attempts to make before giving up, including the first.\n\n###  `` `jitter: bool = True` `class-attribute` `instance-attribute` \u00b6\n\nWhether to add random jitter to the interval between retries.\n\n###  `` `retry_on: Union[Type[Exception], Sequence[Type[Exception]], Callable[[Exception], bool]] = default_retry_on` `class-attribute` `instance-attribute` \u00b6\n\nList of exception classes that should trigger a retry, or a callable that returns True for exceptions that should trigger a retry.\n\n##  `` `CachePolicy` \u00b6\n\nBases: `NamedTuple`\n\nConfiguration for caching nodes.\n\n##  `` `Interrupt` `dataclass` \u00b6\n\n##  `` `PregelTask` \u00b6\n\nBases: `NamedTuple`\n\n##  `` `PregelExecutableTask` \u00b6\n\nBases: `NamedTuple`\n\n##  `` `StateSnapshot` \u00b6\n\nBases: `NamedTuple`\n\nSnapshot of the state of the graph at the beginning of a step.\n\n###  `` `values: Union[dict[str, Any], Any]` `instance-attribute` \u00b6\n\nCurrent values of channels\n\n###  `` `next: tuple[str, ...]` `instance-attribute` \u00b6\n\nThe name of the node to execute in each task for this step.\n\n###  `` `config: RunnableConfig` `instance-attribute` \u00b6\n\nConfig used to fetch this snapshot\n\n###  `` `metadata: Optional[CheckpointMetadata]` `instance-attribute` \u00b6\n\nMetadata associated with this snapshot\n\n###  `` `created_at: Optional[str]` `instance-attribute` \u00b6\n\nTimestamp of snapshot creation\n\n###  `` `parent_config: Optional[RunnableConfig]` `instance-attribute` \u00b6\n\nConfig used to fetch the parent snapshot, if any\n\n###  `` `tasks: tuple[PregelTask, ...]` `instance-attribute` \u00b6\n\nTasks to execute in this step. If already attempted, may contain an error.\n\n##  `` `Send` \u00b6\n\nA message or packet to send to a specific node in the graph.\n\nThe `Send` class is used within a `StateGraph`'s conditional edges to dynamically invoke a node with a custom state at the next step.\n\nImportantly, the sent state can differ from the core graph's state, allowing for flexible and dynamic workflow management.\n\nOne such example is a \"map-reduce\" workflow where your graph invokes the same node multiple times in parallel with different states, before aggregating the results back into the main graph's state.\n\nAttributes:\n\n  * **`node`** (`str`) \u2013 \n\nThe name of the target node to send the message to.\n\n  * **`arg`** (`Any`) \u2013 \n\nThe state or message to send to the target node.\n\n\n\n\nExamples:\n[code] \n    >>> from typing import Annotated\n    >>> import operator\n    >>> class OverallState(TypedDict):\n    ...     subjects: list[str]\n    ...     jokes: Annotated[list[str], operator.add]\n    ...\n    >>> from langgraph.types import Send\n    >>> from langgraph.graph import END, START\n    >>> def continue_to_jokes(state: OverallState):\n    ...     return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n    ...\n    >>> from langgraph.graph import StateGraph\n    >>> builder = StateGraph(OverallState)\n    >>> builder.add_node(\"generate_joke\", lambda state: {\"jokes\": [f\"Joke about {state['subject']}\"]})\n    >>> builder.add_conditional_edges(START, continue_to_jokes)\n    >>> builder.add_edge(\"generate_joke\", END)\n    >>> graph = builder.compile()\n    >>>\n    >>> # Invoking with two subjects results in a generated joke for each\n    >>> graph.invoke({\"subjects\": [\"cats\", \"dogs\"]})\n    {'subjects': ['cats', 'dogs'], 'jokes': ['Joke about cats', 'Joke about dogs']}\n    \n[/code]\n\n###  `` `__init__(node: str, arg: Any) -> None` \u00b6\n\nInitialize a new instance of the Send class.\n\nParameters:\n\n  * **`node`** (`str`) \u2013 \n\nThe name of the target node to send the message to.\n\n  * **`arg`** (`Any`) \u2013 \n\nThe state or message to send to the target node.\n\n\n\n\n##  `` `Command` `dataclass` \u00b6\n\nBases: `Generic[N]`, `ToolOutputMixin`\n\nOne or more commands to update the graph's state and send messages to nodes.\n\nParameters:\n\n  * **`graph`** (`Optional[str]`, default: `None` ) \u2013 \n\ngraph to send the command to. Supported values are:\n\n    * None: the current graph (default)\n    * Command.PARENT: closest parent graph\n\n  * **`update`** (`Optional[Any]`, default: `None` ) \u2013 \n\nupdate to apply to the graph's state.\n\n  * **`resume`** (`Optional[Union[Any, dict[str, Any]]]`, default: `None` ) \u2013 \n\nvalue to resume execution with. To be used together with `interrupt()`.\n\n  * **`goto`** (`Union[Send, Sequence[Union[Send, str]], str]`, default: `()` ) \u2013 \n\ncan be one of the following:\n\n    * name of the node to navigate to next (any node that belongs to the specified `graph`)\n    * sequence of node names to navigate to next\n    * `Send` object (to execute a node with the input provided)\n    * sequence of `Send` objects\n\n\n\n\n##  `` `interrupt(value: Any) -> Any` \u00b6\n\nInterrupt the graph with a resumable exception from within a node.\n\nThe `interrupt` function enables human-in-the-loop workflows by pausing graph execution and surfacing a value to the client. This value can communicate context or request input required to resume execution.\n\nIn a given node, the first invocation of this function raises a `GraphInterrupt` exception, halting execution. The provided `value` is included with the exception and sent to the client executing the graph.\n\nA client resuming the graph must use the `Command` primitive to specify a value for the interrupt and continue execution. The graph resumes from the start of the node, **re-executing** all logic.\n\nIf a node contains multiple `interrupt` calls, LangGraph matches resume values to interrupts based on their order in the node. This list of resume values is scoped to the specific task executing the node and is not shared across tasks.\n\nTo use an `interrupt`, you must enable a checkpointer, as the feature relies on persisting the graph state.\n\nExample\n[code] \n    import uuid\n    from typing import Optional\n    from typing_extensions import TypedDict\n    \n    from langgraph.checkpoint.memory import MemorySaver\n    from langgraph.constants import START\n    from langgraph.graph import StateGraph\n    from langgraph.types import interrupt\n    \n    \n    class State(TypedDict):\n        \"\"\"The graph state.\"\"\"\n    \n        foo: str\n        human_value: Optional[str]\n        \"\"\"Human value will be updated using an interrupt.\"\"\"\n    \n    \n    def node(state: State):\n        answer = interrupt(\n            # This value will be sent to the client\n            # as part of the interrupt information.\n            \"what is your age?\"\n        )\n        print(f\"> Received an input from the interrupt: {answer}\")\n        return {\"human_value\": answer}\n    \n    \n    builder = StateGraph(State)\n    builder.add_node(\"node\", node)\n    builder.add_edge(START, \"node\")\n    \n    # A checkpointer must be enabled for interrupts to work!\n    checkpointer = MemorySaver()\n    graph = builder.compile(checkpointer=checkpointer)\n    \n    config = {\n        \"configurable\": {\n            \"thread_id\": uuid.uuid4(),\n        }\n    }\n    \n    for chunk in graph.stream({\"foo\": \"abc\"}, config):\n        print(chunk)\n    \n[/code]\n[code] \n    {'__interrupt__': (Interrupt(value='what is your age?', resumable=True, ns=['node:62e598fa-8653-9d6d-2046-a70203020e37'], when='during'),)}\n    \n[/code]\n[code] \n    command = Command(resume=\"some input from a human!!!\")\n    \n    for chunk in graph.stream(Command(resume=\"some input from a human!!!\"), config):\n        print(chunk)\n    \n[/code]\n[code] \n    Received an input from the interrupt: some input from a human!!!\n    {'node': {'human_value': 'some input from a human!!!'}}\n    \n[/code]\n\nParameters:\n\n  * **`value`** (`Any`) \u2013 \n\nThe value to surface to the client when the graph is interrupted.\n\n\n\n\nReturns:\n\n  * **`Any`** ( `Any` ) \u2013 \n\nOn subsequent invocations within the same node (same task to be precise), returns the value provided during the first invocation\n\n\n\n\nRaises:\n\n  * `GraphInterrupt` \u2013 \n\nOn the first invocation within the node, halts execution and surfaces the provided value to the client.\n\n\n\n\n## Comments\n",
    "metadata": {
        "url": "https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command",
        "title": "Types",
        "description": "Build language agents as graphs",
        "keywords": "No keywords"
    }
}