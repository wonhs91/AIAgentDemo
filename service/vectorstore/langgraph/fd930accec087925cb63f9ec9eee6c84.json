{
    "page_content": "# Graph Definitions\u00b6\n\n##  `` `Graph` \u00b6\n\n###  `` `add_conditional_edges(source: str, path: Union[Callable[..., Union[Hashable, list[Hashable]]], Callable[..., Awaitable[Union[Hashable, list[Hashable]]]], Runnable[Any, Union[Hashable, list[Hashable]]]], path_map: Optional[Union[dict[Hashable, str], list[str]]] = None, then: Optional[str] = None) -> Self` \u00b6\n\nAdd a conditional edge from the starting node to any number of destination nodes.\n\nParameters:\n\n  * **`source`** (`str`) \u2013 \n\nThe starting node. This conditional edge will run when exiting this node.\n\n  * **`path`** (`Union[Callable, Runnable]`) \u2013 \n\nThe callable that determines the next node or nodes. If not specifying `path_map` it should return one or more nodes. If it returns END, the graph will stop execution.\n\n  * **`path_map`** (`Optional[dict[Hashable, str]]`, default: `None` ) \u2013 \n\nOptional mapping of paths to node names. If omitted the paths returned by `path` should be node names.\n\n  * **`then`** (`Optional[str]`, default: `None` ) \u2013 \n\nThe name of a node to execute after the nodes selected by `path`.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\nWithout typehints on the `path` function's return value (e.g., `-> Literal[\"foo\", \"__end__\"]:`)\n\nor a path_map, the graph visualization assumes the edge could transition to any node in the graph.\n\n###  `` `set_entry_point(key: str) -> Self` \u00b6\n\nSpecifies the first node to be called in the graph.\n\nEquivalent to calling `add_edge(START, key)`.\n\nParameters:\n\n  * **`key`** (`str`) \u2013 \n\nThe key of the node to set as the entry point.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n###  `` `set_conditional_entry_point(path: Union[Callable[..., Union[Hashable, list[Hashable]]], Callable[..., Awaitable[Union[Hashable, list[Hashable]]]], Runnable[Any, Union[Hashable, list[Hashable]]]], path_map: Optional[Union[dict[Hashable, str], list[str]]] = None, then: Optional[str] = None) -> Self` \u00b6\n\nSets a conditional entry point in the graph.\n\nParameters:\n\n  * **`path`** (`Union[Callable, Runnable]`) \u2013 \n\nThe callable that determines the next node or nodes. If not specifying `path_map` it should return one or more nodes. If it returns END, the graph will stop execution.\n\n  * **`path_map`** (`Optional[dict[str, str]]`, default: `None` ) \u2013 \n\nOptional mapping of paths to node names. If omitted the paths returned by `path` should be node names.\n\n  * **`then`** (`Optional[str]`, default: `None` ) \u2013 \n\nThe name of a node to execute after the nodes selected by `path`.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n###  `` `set_finish_point(key: str) -> Self` \u00b6\n\nMarks a node as a finish point of the graph.\n\nIf the graph reaches this node, it will cease execution.\n\nParameters:\n\n  * **`key`** (`str`) \u2013 \n\nThe key of the node to set as the finish point.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n##  `` `CompiledGraph` \u00b6\n\nBases: `Pregel`\n\n###  `` `stream_mode: StreamMode = stream_mode` `class-attribute` `instance-attribute` \u00b6\n\nMode to stream output, defaults to 'values'.\n\n###  `` `stream_eager: bool = stream_eager` `class-attribute` `instance-attribute` \u00b6\n\nWhether to force emitting stream events eagerly, automatically turned on for stream_mode \"messages\" and \"custom\".\n\n###  `` `stream_channels: Optional[Union[str, Sequence[str]]] = stream_channels` `class-attribute` `instance-attribute` \u00b6\n\nChannels to stream, defaults to all channels not in reserved channels\n\n###  `` `step_timeout: Optional[float] = step_timeout` `class-attribute` `instance-attribute` \u00b6\n\nMaximum time to wait for a step to complete, in seconds. Defaults to None.\n\n###  `` `debug: bool = debug if debug is not None else get_debug()` `instance-attribute` \u00b6\n\nWhether to print debug information during execution. Defaults to False.\n\n###  `` `checkpointer: Checkpointer = checkpointer` `class-attribute` `instance-attribute` \u00b6\n\nCheckpointer used to save and load graph state. Defaults to None.\n\n###  `` `store: Optional[BaseStore] = store` `class-attribute` `instance-attribute` \u00b6\n\nMemory store to use for SharedValues. Defaults to None.\n\n###  `` `retry_policy: Optional[RetryPolicy] = retry_policy` `class-attribute` `instance-attribute` \u00b6\n\nRetry policy to use when running tasks. Set to None to disable.\n\n###  `` `get_state(config: RunnableConfig, *, subgraphs: bool = False) -> StateSnapshot` \u00b6\n\nGet the current state of the graph.\n\n###  `` `aget_state(config: RunnableConfig, *, subgraphs: bool = False) -> StateSnapshot` `async` \u00b6\n\nGet the current state of the graph.\n\n###  `` `update_state(config: RunnableConfig, values: Optional[Union[dict[str, Any], Any]], as_node: Optional[str] = None) -> RunnableConfig` \u00b6\n\nUpdate the state of the graph with the given values, as if they came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous.\n\n###  `` `aupdate_state(config: RunnableConfig, values: dict[str, Any] | Any, as_node: Optional[str] = None) -> RunnableConfig` `async` \u00b6\n\nUpdate the state of the graph asynchronously with the given values, as if they came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous.\n\n###  `` `stream(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None, output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, subgraphs: bool = False) -> Iterator[Union[dict[str, Any], Any]]` \u00b6\n\nStream graph steps for a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input to the graph.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nThe configuration to use for the run.\n\n  * **`stream_mode`** (`Optional[Union[StreamMode, list[StreamMode]]]`, default: `None` ) \u2013 \n\nThe mode to stream output, defaults to self.stream_mode. Options are:\n\n    * `\"values\"`: Emit all values in the state after each step. When used with functional API, values are emitted once at the end of the workflow.\n    * `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n    * `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n    * `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n    * `\"debug\"`: Emit debug events with as much information as possible for each step.\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nThe keys to stream, defaults to all non-context channels.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt before, defaults to all nodes in the graph.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt after, defaults to all nodes in the graph.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nWhether to print debug information during execution, defaults to False.\n\n  * **`subgraphs`** (`bool`, default: `False` ) \u2013 \n\nWhether to stream subgraphs, defaults to False.\n\n\n\n\nYields:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe output of each step in the graph. The output shape depends on the stream_mode.\n\n\n\n\nExamples:\n\nUsing different stream modes with a graph: \n[code]\n    >>> import operator\n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    ...\n    >>> class State(TypedDict):\n    ...     alist: Annotated[list, operator.add]\n    ...     another_list: Annotated[list, operator.add]\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", lambda _state: {\"another_list\": [\"hi\"]})\n    >>> builder.add_node(\"b\", lambda _state: {\"alist\": [\"there\"]})\n    >>> builder.add_edge(\"a\", \"b\")\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n[/code]\n\nWith stream_mode=\"values\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"values\"']}, stream_mode=\"values\"):\n    ...     print(event)\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': []}\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': ['hi']}\n    {'alist': ['Ex for stream_mode=\"values\"', 'there'], 'another_list': ['hi']}\n    \n[/code]\n\nWith stream_mode=\"updates\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"updates\"']}, stream_mode=\"updates\"):\n    ...     print(event)\n    {'a': {'another_list': ['hi']}}\n    {'b': {'alist': ['there']}}\n    \n[/code]\n\nWith stream_mode=\"debug\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"debug\"']}, stream_mode=\"debug\"):\n    ...     print(event)\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': []}, 'triggers': ['start:a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': ['hi']}, 'triggers': ['a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}\n    \n[/code]\n\nWith stream_mode=\"custom\":\n[code] \n    >>> from langgraph.types import StreamWriter\n    ...\n    >>> def node_a(state: State, writer: StreamWriter):\n    ...     writer({\"custom_data\": \"foo\"})\n    ...     return {\"alist\": [\"hi\"]}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    ...\n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"custom\"']}, stream_mode=\"custom\"):\n    ...     print(event)\n    {'custom_data': 'foo'}\n    \n[/code]\n\nWith stream_mode=\"messages\":\n[code] \n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    >>> from langchain_openai import ChatOpenAI\n    ...\n    >>> llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ...\n    >>> class State(TypedDict):\n    ...     question: str\n    ...     answer: str\n    ...\n    >>> def node_a(state: State):\n    ...     response = llm.invoke(state[\"question\"])\n    ...     return {\"answer\": response.content}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n    >>> for event in graph.stream({\"question\": \"What is the capital of France?\"}, stream_mode=\"messages\"):\n    ...     print(event)\n    (AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], 'langgraph_path': ('__pregel_pull', 'a'), 'langgraph_checkpoint_ns': '...', 'checkpoint_ns': '...', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n    (AIMessageChunk(content=' capital', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], ...})\n    (AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' France', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' Paris', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    \n[/code]\n\n###  `` `astream(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None, output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, subgraphs: bool = False) -> AsyncIterator[Union[dict[str, Any], Any]]` `async` \u00b6\n\nStream graph steps for a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input to the graph.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nThe configuration to use for the run.\n\n  * **`stream_mode`** (`Optional[Union[StreamMode, list[StreamMode]]]`, default: `None` ) \u2013 \n\nThe mode to stream output, defaults to self.stream_mode. Options are:\n\n    * `\"values\"`: Emit all values in the state after each step. When used with functional API, values are emitted once at the end of the workflow.\n    * `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n    * `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n    * `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n    * `\"debug\"`: Emit debug events with as much information as possible for each step.\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nThe keys to stream, defaults to all non-context channels.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt before, defaults to all nodes in the graph.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt after, defaults to all nodes in the graph.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nWhether to print debug information during execution, defaults to False.\n\n  * **`subgraphs`** (`bool`, default: `False` ) \u2013 \n\nWhether to stream subgraphs, defaults to False.\n\n\n\n\nYields:\n\n  * `AsyncIterator[Union[dict[str, Any], Any]]` \u2013 \n\nThe output of each step in the graph. The output shape depends on the stream_mode.\n\n\n\n\nExamples:\n\nUsing different stream modes with a graph: \n[code]\n    >>> import operator\n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    ...\n    >>> class State(TypedDict):\n    ...     alist: Annotated[list, operator.add]\n    ...     another_list: Annotated[list, operator.add]\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", lambda _state: {\"another_list\": [\"hi\"]})\n    >>> builder.add_node(\"b\", lambda _state: {\"alist\": [\"there\"]})\n    >>> builder.add_edge(\"a\", \"b\")\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n[/code]\n\nWith stream_mode=\"values\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"values\"']}, stream_mode=\"values\"):\n    ...     print(event)\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': []}\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': ['hi']}\n    {'alist': ['Ex for stream_mode=\"values\"', 'there'], 'another_list': ['hi']}\n    \n[/code]\n\nWith stream_mode=\"updates\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"updates\"']}, stream_mode=\"updates\"):\n    ...     print(event)\n    {'a': {'another_list': ['hi']}}\n    {'b': {'alist': ['there']}}\n    \n[/code]\n\nWith stream_mode=\"debug\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"debug\"']}, stream_mode=\"debug\"):\n    ...     print(event)\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': []}, 'triggers': ['start:a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': ['hi']}, 'triggers': ['a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}\n    \n[/code]\n\nWith stream_mode=\"custom\":\n[code] \n    >>> from langgraph.types import StreamWriter\n    ...\n    >>> async def node_a(state: State, writer: StreamWriter):\n    ...     writer({\"custom_data\": \"foo\"})\n    ...     return {\"alist\": [\"hi\"]}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    ...\n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"custom\"']}, stream_mode=\"custom\"):\n    ...     print(event)\n    {'custom_data': 'foo'}\n    \n[/code]\n\nWith stream_mode=\"messages\":\n[code] \n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    >>> from langchain_openai import ChatOpenAI\n    ...\n    >>> llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ...\n    >>> class State(TypedDict):\n    ...     question: str\n    ...     answer: str\n    ...\n    >>> async def node_a(state: State):\n    ...     response = await llm.ainvoke(state[\"question\"])\n    ...     return {\"answer\": response.content}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n    >>> for event in graph.stream({\"question\": \"What is the capital of France?\"}, stream_mode=\"messages\"):\n    ...     print(event)\n    (AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], 'langgraph_path': ('__pregel_pull', 'a'), 'langgraph_checkpoint_ns': '...', 'checkpoint_ns': '...', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n    (AIMessageChunk(content=' capital', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], ...})\n    (AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' France', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' Paris', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    \n[/code]\n\n###  `` `invoke(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: StreamMode = 'values', output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, **kwargs: Any) -> Union[dict[str, Any], Any]` \u00b6\n\nRun the graph with a single input and config.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input data for the graph. It can be a dictionary or any other type.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nOptional. The configuration for the graph run.\n\n  * **`stream_mode`** (`StreamMode`, default: `'values'` ) \u2013 \n\nOptional[str]. The stream mode for the graph run. Default is \"values\".\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The output keys to retrieve from the graph run.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt the graph run before.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt the graph run after.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nOptional. Enable debug mode for the graph run.\n\n  * **`**kwargs`** (`Any`, default: `{}` ) \u2013 \n\nAdditional keyword arguments to pass to the graph run.\n\n\n\n\nReturns:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe output of the graph run. If stream_mode is \"values\", it returns the latest output.\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nIf stream_mode is not \"values\", it returns a list of output chunks.\n\n\n\n\n###  `` `ainvoke(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: StreamMode = 'values', output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, **kwargs: Any) -> Union[dict[str, Any], Any]` `async` \u00b6\n\nAsynchronously invoke the graph on a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input data for the computation. It can be a dictionary or any other type.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nOptional. The configuration for the computation.\n\n  * **`stream_mode`** (`StreamMode`, default: `'values'` ) \u2013 \n\nOptional. The stream mode for the computation. Default is \"values\".\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The output keys to include in the result. Default is None.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt before. Default is None.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt after. Default is None.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nOptional. Whether to enable debug mode. Default is None.\n\n  * **`**kwargs`** (`Any`, default: `{}` ) \u2013 \n\nAdditional keyword arguments.\n\n\n\n\nReturns:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe result of the computation. If stream_mode is \"values\", it returns the latest value.\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nIf stream_mode is \"chunks\", it returns a list of chunks.\n\n\n\n\n###  `` `get_graph(config: Optional[RunnableConfig] = None, *, xray: Union[int, bool] = False) -> DrawableGraph` \u00b6\n\nReturns a drawable representation of the computation graph.\n\n##  `` `StateGraph` \u00b6\n\nBases: `Graph`\n\nA graph whose nodes communicate by reading and writing to a shared state. The signature of each node is State -> Partial.\n\nEach state key can optionally be annotated with a reducer function that will be used to aggregate the values of that key received from multiple nodes. The signature of a reducer function is (Value, Value) -> Value.\n\nParameters:\n\n  * **`state_schema`** (`Type[Any]`, default: `None` ) \u2013 \n\nThe schema class that defines the state.\n\n  * **`config_schema`** (`Optional[Type[Any]]`, default: `None` ) \u2013 \n\nThe schema class that defines the configuration. Use this to expose configurable parameters in your API.\n\n\n\n\nExamples:\n[code] \n    >>> from langchain_core.runnables import RunnableConfig\n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.checkpoint.memory import MemorySaver\n    >>> from langgraph.graph import StateGraph\n    >>>\n    >>> def reducer(a: list, b: int | None) -> list:\n    ...     if b is not None:\n    ...         return a + [b]\n    ...     return a\n    >>>\n    >>> class State(TypedDict):\n    ...     x: Annotated[list, reducer]\n    >>>\n    >>> class ConfigSchema(TypedDict):\n    ...     r: float\n    >>>\n    >>> graph = StateGraph(State, config_schema=ConfigSchema)\n    >>>\n    >>> def node(state: State, config: RunnableConfig) -> dict:\n    ...     r = config[\"configurable\"].get(\"r\", 1.0)\n    ...     x = state[\"x\"][-1]\n    ...     next_value = x * r * (1 - x)\n    ...     return {\"x\": next_value}\n    >>>\n    >>> graph.add_node(\"A\", node)\n    >>> graph.set_entry_point(\"A\")\n    >>> graph.set_finish_point(\"A\")\n    >>> compiled = graph.compile()\n    >>>\n    >>> print(compiled.config_specs)\n    [ConfigurableFieldSpec(id='r', annotation=<class 'float'>, name=None, description=None, default=None, is_shared=False, dependencies=None)]\n    >>>\n    >>> step1 = compiled.invoke({\"x\": 0.5}, {\"configurable\": {\"r\": 3.0}})\n    >>> print(step1)\n    {'x': [0.5, 0.75]}\n    \n[/code]\n\n###  `` `add_conditional_edges(source: str, path: Union[Callable[..., Union[Hashable, list[Hashable]]], Callable[..., Awaitable[Union[Hashable, list[Hashable]]]], Runnable[Any, Union[Hashable, list[Hashable]]]], path_map: Optional[Union[dict[Hashable, str], list[str]]] = None, then: Optional[str] = None) -> Self` \u00b6\n\nAdd a conditional edge from the starting node to any number of destination nodes.\n\nParameters:\n\n  * **`source`** (`str`) \u2013 \n\nThe starting node. This conditional edge will run when exiting this node.\n\n  * **`path`** (`Union[Callable, Runnable]`) \u2013 \n\nThe callable that determines the next node or nodes. If not specifying `path_map` it should return one or more nodes. If it returns END, the graph will stop execution.\n\n  * **`path_map`** (`Optional[dict[Hashable, str]]`, default: `None` ) \u2013 \n\nOptional mapping of paths to node names. If omitted the paths returned by `path` should be node names.\n\n  * **`then`** (`Optional[str]`, default: `None` ) \u2013 \n\nThe name of a node to execute after the nodes selected by `path`.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\nWithout typehints on the `path` function's return value (e.g., `-> Literal[\"foo\", \"__end__\"]:`)\n\nor a path_map, the graph visualization assumes the edge could transition to any node in the graph.\n\n###  `` `set_entry_point(key: str) -> Self` \u00b6\n\nSpecifies the first node to be called in the graph.\n\nEquivalent to calling `add_edge(START, key)`.\n\nParameters:\n\n  * **`key`** (`str`) \u2013 \n\nThe key of the node to set as the entry point.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n###  `` `set_conditional_entry_point(path: Union[Callable[..., Union[Hashable, list[Hashable]]], Callable[..., Awaitable[Union[Hashable, list[Hashable]]]], Runnable[Any, Union[Hashable, list[Hashable]]]], path_map: Optional[Union[dict[Hashable, str], list[str]]] = None, then: Optional[str] = None) -> Self` \u00b6\n\nSets a conditional entry point in the graph.\n\nParameters:\n\n  * **`path`** (`Union[Callable, Runnable]`) \u2013 \n\nThe callable that determines the next node or nodes. If not specifying `path_map` it should return one or more nodes. If it returns END, the graph will stop execution.\n\n  * **`path_map`** (`Optional[dict[str, str]]`, default: `None` ) \u2013 \n\nOptional mapping of paths to node names. If omitted the paths returned by `path` should be node names.\n\n  * **`then`** (`Optional[str]`, default: `None` ) \u2013 \n\nThe name of a node to execute after the nodes selected by `path`.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n###  `` `set_finish_point(key: str) -> Self` \u00b6\n\nMarks a node as a finish point of the graph.\n\nIf the graph reaches this node, it will cease execution.\n\nParameters:\n\n  * **`key`** (`str`) \u2013 \n\nThe key of the node to set as the finish point.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the graph, allowing for method chaining.\n\n\n\n\n###  `` `add_node(node: Union[str, RunnableLike], action: Optional[RunnableLike] = None, *, metadata: Optional[dict[str, Any]] = None, input: Optional[Type[Any]] = None, retry: Optional[RetryPolicy] = None) -> Self` \u00b6\n\nAdds a new node to the state graph.\n\nWill take the name of the function/runnable as the node name.\n\nParameters:\n\n  * **`node`** (`Union[str, RunnableLike)]`) \u2013 \n\nThe function or runnable this node will run.\n\n  * **`action`** (`Optional[RunnableLike]`, default: `None` ) \u2013 \n\nThe action associated with the node. (default: None)\n\n  * **`metadata`** (`Optional[dict[str, Any]]`, default: `None` ) \u2013 \n\nThe metadata associated with the node. (default: None)\n\n  * **`input`** (`Optional[Type[Any]]`, default: `None` ) \u2013 \n\nThe input schema for the node. (default: the graph's input schema)\n\n  * **`retry`** (`Optional[RetryPolicy]`, default: `None` ) \u2013 \n\nThe policy for retrying the node. (default: None)\n\n\n\n\nRaises:\n\n  * `ValueError` \u2013 \n\nIf the key is already being used as a state key.\n\n\n\n\nExamples:\n[code] \n    >>> from langgraph.graph import START, StateGraph\n    ...\n    >>> def my_node(state, config):\n    ...    return {\"x\": state[\"x\"] + 1}\n    ...\n    >>> builder = StateGraph(dict)\n    >>> builder.add_node(my_node)  # node name will be 'my_node'\n    >>> builder.add_edge(START, \"my_node\")\n    >>> graph = builder.compile()\n    >>> graph.invoke({\"x\": 1})\n    {'x': 2}\n    \n[/code]\n\nCustomize the name:\n[code] \n    >>> builder = StateGraph(dict)\n    >>> builder.add_node(\"my_fair_node\", my_node)\n    >>> builder.add_edge(START, \"my_fair_node\")\n    >>> graph = builder.compile()\n    >>> graph.invoke({\"x\": 1})\n    {'x': 2}\n    \n[/code]\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the state graph, allowing for method chaining.\n\n\n\n\n###  `` `add_edge(start_key: Union[str, list[str]], end_key: str) -> Self` \u00b6\n\nAdds a directed edge from the start node (or list of start nodes) to the end node.\n\nWhen a single start node is provided, the graph will wait for that node to complete before executing the end node. When multiple start nodes are provided, the graph will wait for ALL of the start nodes to complete before executing the end node.\n\nParameters:\n\n  * **`start_key`** (`Union[str, list[str]]`) \u2013 \n\nThe key(s) of the start node(s) of the edge.\n\n  * **`end_key`** (`str`) \u2013 \n\nThe key of the end node of the edge.\n\n\n\n\nRaises:\n\n  * `ValueError` \u2013 \n\nIf the start key is 'END' or if the start key or end key is not present in the graph.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the state graph, allowing for method chaining.\n\n\n\n\n###  `` `add_sequence(nodes: Sequence[Union[RunnableLike, tuple[str, RunnableLike]]]) -> Self` \u00b6\n\nAdd a sequence of nodes that will be executed in the provided order.\n\nParameters:\n\n  * **`nodes`** (`Sequence[Union[RunnableLike, tuple[str, RunnableLike]]]`) \u2013 \n\nA sequence of RunnableLike objects (e.g. a LangChain Runnable or a callable) or (name, RunnableLike) tuples. If no names are provided, the name will be inferred from the node object (e.g. a runnable or a callable name). Each node will be executed in the order provided.\n\n\n\n\nRaises:\n\n  * `ValueError` \u2013 \n\nif the sequence is empty.\n\n  * `ValueError` \u2013 \n\nif the sequence contains duplicate node names.\n\n\n\n\nReturns:\n\n  * **`Self`** ( `Self` ) \u2013 \n\nThe instance of the state graph, allowing for method chaining.\n\n\n\n\n###  `` `compile(checkpointer: Checkpointer = None, *, store: Optional[BaseStore] = None, interrupt_before: Optional[Union[All, list[str]]] = None, interrupt_after: Optional[Union[All, list[str]]] = None, debug: bool = False, name: Optional[str] = None) -> CompiledStateGraph` \u00b6\n\nCompiles the state graph into a `CompiledGraph` object.\n\nThe compiled graph implements the `Runnable` interface and can be invoked, streamed, batched, and run asynchronously.\n\nParameters:\n\n  * **`checkpointer`** (`Optional[Union[Checkpointer, Literal[False]]]`, default: `None` ) \u2013 \n\nA checkpoint saver object or flag. If provided, this Checkpointer serves as a fully versioned \"short-term memory\" for the graph, allowing it to be paused, resumed, and replayed from any point. If None, it may inherit the parent graph's checkpointer when used as a subgraph. If False, it will not use or inherit any checkpointer.\n\n  * **`interrupt_before`** (`Optional[Sequence[str]]`, default: `None` ) \u2013 \n\nAn optional list of node names to interrupt before.\n\n  * **`interrupt_after`** (`Optional[Sequence[str]]`, default: `None` ) \u2013 \n\nAn optional list of node names to interrupt after.\n\n  * **`debug`** (`bool`, default: `False` ) \u2013 \n\nA flag indicating whether to enable debug mode.\n\n\n\n\nReturns:\n\n  * **`CompiledStateGraph`** ( `CompiledStateGraph` ) \u2013 \n\nThe compiled state graph.\n\n\n\n\n##  `` `CompiledStateGraph` \u00b6\n\nBases: `CompiledGraph`\n\n###  `` `stream_mode: StreamMode = stream_mode` `class-attribute` `instance-attribute` \u00b6\n\nMode to stream output, defaults to 'values'.\n\n###  `` `stream_eager: bool = stream_eager` `class-attribute` `instance-attribute` \u00b6\n\nWhether to force emitting stream events eagerly, automatically turned on for stream_mode \"messages\" and \"custom\".\n\n###  `` `stream_channels: Optional[Union[str, Sequence[str]]] = stream_channels` `class-attribute` `instance-attribute` \u00b6\n\nChannels to stream, defaults to all channels not in reserved channels\n\n###  `` `step_timeout: Optional[float] = step_timeout` `class-attribute` `instance-attribute` \u00b6\n\nMaximum time to wait for a step to complete, in seconds. Defaults to None.\n\n###  `` `debug: bool = debug if debug is not None else get_debug()` `instance-attribute` \u00b6\n\nWhether to print debug information during execution. Defaults to False.\n\n###  `` `checkpointer: Checkpointer = checkpointer` `class-attribute` `instance-attribute` \u00b6\n\nCheckpointer used to save and load graph state. Defaults to None.\n\n###  `` `store: Optional[BaseStore] = store` `class-attribute` `instance-attribute` \u00b6\n\nMemory store to use for SharedValues. Defaults to None.\n\n###  `` `retry_policy: Optional[RetryPolicy] = retry_policy` `class-attribute` `instance-attribute` \u00b6\n\nRetry policy to use when running tasks. Set to None to disable.\n\n###  `` `get_graph(config: Optional[RunnableConfig] = None, *, xray: Union[int, bool] = False) -> DrawableGraph` \u00b6\n\nReturns a drawable representation of the computation graph.\n\n###  `` `get_state(config: RunnableConfig, *, subgraphs: bool = False) -> StateSnapshot` \u00b6\n\nGet the current state of the graph.\n\n###  `` `aget_state(config: RunnableConfig, *, subgraphs: bool = False) -> StateSnapshot` `async` \u00b6\n\nGet the current state of the graph.\n\n###  `` `update_state(config: RunnableConfig, values: Optional[Union[dict[str, Any], Any]], as_node: Optional[str] = None) -> RunnableConfig` \u00b6\n\nUpdate the state of the graph with the given values, as if they came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous.\n\n###  `` `aupdate_state(config: RunnableConfig, values: dict[str, Any] | Any, as_node: Optional[str] = None) -> RunnableConfig` `async` \u00b6\n\nUpdate the state of the graph asynchronously with the given values, as if they came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous.\n\n###  `` `stream(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None, output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, subgraphs: bool = False) -> Iterator[Union[dict[str, Any], Any]]` \u00b6\n\nStream graph steps for a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input to the graph.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nThe configuration to use for the run.\n\n  * **`stream_mode`** (`Optional[Union[StreamMode, list[StreamMode]]]`, default: `None` ) \u2013 \n\nThe mode to stream output, defaults to self.stream_mode. Options are:\n\n    * `\"values\"`: Emit all values in the state after each step. When used with functional API, values are emitted once at the end of the workflow.\n    * `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n    * `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n    * `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n    * `\"debug\"`: Emit debug events with as much information as possible for each step.\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nThe keys to stream, defaults to all non-context channels.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt before, defaults to all nodes in the graph.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt after, defaults to all nodes in the graph.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nWhether to print debug information during execution, defaults to False.\n\n  * **`subgraphs`** (`bool`, default: `False` ) \u2013 \n\nWhether to stream subgraphs, defaults to False.\n\n\n\n\nYields:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe output of each step in the graph. The output shape depends on the stream_mode.\n\n\n\n\nExamples:\n\nUsing different stream modes with a graph: \n[code]\n    >>> import operator\n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    ...\n    >>> class State(TypedDict):\n    ...     alist: Annotated[list, operator.add]\n    ...     another_list: Annotated[list, operator.add]\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", lambda _state: {\"another_list\": [\"hi\"]})\n    >>> builder.add_node(\"b\", lambda _state: {\"alist\": [\"there\"]})\n    >>> builder.add_edge(\"a\", \"b\")\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n[/code]\n\nWith stream_mode=\"values\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"values\"']}, stream_mode=\"values\"):\n    ...     print(event)\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': []}\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': ['hi']}\n    {'alist': ['Ex for stream_mode=\"values\"', 'there'], 'another_list': ['hi']}\n    \n[/code]\n\nWith stream_mode=\"updates\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"updates\"']}, stream_mode=\"updates\"):\n    ...     print(event)\n    {'a': {'another_list': ['hi']}}\n    {'b': {'alist': ['there']}}\n    \n[/code]\n\nWith stream_mode=\"debug\":\n[code] \n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"debug\"']}, stream_mode=\"debug\"):\n    ...     print(event)\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': []}, 'triggers': ['start:a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': ['hi']}, 'triggers': ['a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}\n    \n[/code]\n\nWith stream_mode=\"custom\":\n[code] \n    >>> from langgraph.types import StreamWriter\n    ...\n    >>> def node_a(state: State, writer: StreamWriter):\n    ...     writer({\"custom_data\": \"foo\"})\n    ...     return {\"alist\": [\"hi\"]}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    ...\n    >>> for event in graph.stream({\"alist\": ['Ex for stream_mode=\"custom\"']}, stream_mode=\"custom\"):\n    ...     print(event)\n    {'custom_data': 'foo'}\n    \n[/code]\n\nWith stream_mode=\"messages\":\n[code] \n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    >>> from langchain_openai import ChatOpenAI\n    ...\n    >>> llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ...\n    >>> class State(TypedDict):\n    ...     question: str\n    ...     answer: str\n    ...\n    >>> def node_a(state: State):\n    ...     response = llm.invoke(state[\"question\"])\n    ...     return {\"answer\": response.content}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n    >>> for event in graph.stream({\"question\": \"What is the capital of France?\"}, stream_mode=\"messages\"):\n    ...     print(event)\n    (AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], 'langgraph_path': ('__pregel_pull', 'a'), 'langgraph_checkpoint_ns': '...', 'checkpoint_ns': '...', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n    (AIMessageChunk(content=' capital', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], ...})\n    (AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' France', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' Paris', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    \n[/code]\n\n###  `` `astream(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None, output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, subgraphs: bool = False) -> AsyncIterator[Union[dict[str, Any], Any]]` `async` \u00b6\n\nStream graph steps for a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input to the graph.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nThe configuration to use for the run.\n\n  * **`stream_mode`** (`Optional[Union[StreamMode, list[StreamMode]]]`, default: `None` ) \u2013 \n\nThe mode to stream output, defaults to self.stream_mode. Options are:\n\n    * `\"values\"`: Emit all values in the state after each step. When used with functional API, values are emitted once at the end of the workflow.\n    * `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n    * `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n    * `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n    * `\"debug\"`: Emit debug events with as much information as possible for each step.\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nThe keys to stream, defaults to all non-context channels.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt before, defaults to all nodes in the graph.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nNodes to interrupt after, defaults to all nodes in the graph.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nWhether to print debug information during execution, defaults to False.\n\n  * **`subgraphs`** (`bool`, default: `False` ) \u2013 \n\nWhether to stream subgraphs, defaults to False.\n\n\n\n\nYields:\n\n  * `AsyncIterator[Union[dict[str, Any], Any]]` \u2013 \n\nThe output of each step in the graph. The output shape depends on the stream_mode.\n\n\n\n\nExamples:\n\nUsing different stream modes with a graph: \n[code]\n    >>> import operator\n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    ...\n    >>> class State(TypedDict):\n    ...     alist: Annotated[list, operator.add]\n    ...     another_list: Annotated[list, operator.add]\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", lambda _state: {\"another_list\": [\"hi\"]})\n    >>> builder.add_node(\"b\", lambda _state: {\"alist\": [\"there\"]})\n    >>> builder.add_edge(\"a\", \"b\")\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n[/code]\n\nWith stream_mode=\"values\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"values\"']}, stream_mode=\"values\"):\n    ...     print(event)\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': []}\n    {'alist': ['Ex for stream_mode=\"values\"'], 'another_list': ['hi']}\n    {'alist': ['Ex for stream_mode=\"values\"', 'there'], 'another_list': ['hi']}\n    \n[/code]\n\nWith stream_mode=\"updates\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"updates\"']}, stream_mode=\"updates\"):\n    ...     print(event)\n    {'a': {'another_list': ['hi']}}\n    {'b': {'alist': ['there']}}\n    \n[/code]\n\nWith stream_mode=\"debug\":\n[code] \n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"debug\"']}, stream_mode=\"debug\"):\n    ...     print(event)\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': []}, 'triggers': ['start:a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}\n    {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=\"debug\"'], 'another_list': ['hi']}, 'triggers': ['a']}}\n    {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}\n    \n[/code]\n\nWith stream_mode=\"custom\":\n[code] \n    >>> from langgraph.types import StreamWriter\n    ...\n    >>> async def node_a(state: State, writer: StreamWriter):\n    ...     writer({\"custom_data\": \"foo\"})\n    ...     return {\"alist\": [\"hi\"]}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    ...\n    >>> async for event in graph.astream({\"alist\": ['Ex for stream_mode=\"custom\"']}, stream_mode=\"custom\"):\n    ...     print(event)\n    {'custom_data': 'foo'}\n    \n[/code]\n\nWith stream_mode=\"messages\":\n[code] \n    >>> from typing_extensions import Annotated, TypedDict\n    >>> from langgraph.graph import StateGraph, START\n    >>> from langchain_openai import ChatOpenAI\n    ...\n    >>> llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ...\n    >>> class State(TypedDict):\n    ...     question: str\n    ...     answer: str\n    ...\n    >>> async def node_a(state: State):\n    ...     response = await llm.ainvoke(state[\"question\"])\n    ...     return {\"answer\": response.content}\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"a\", node_a)\n    >>> builder.add_edge(START, \"a\")\n    >>> graph = builder.compile()\n    \n    >>> for event in graph.stream({\"question\": \"What is the capital of France?\"}, stream_mode=\"messages\"):\n    ...     print(event)\n    (AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], 'langgraph_path': ('__pregel_pull', 'a'), 'langgraph_checkpoint_ns': '...', 'checkpoint_ns': '...', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n    (AIMessageChunk(content=' capital', additional_kwargs={}, response_metadata={}, id='...'), {'langgraph_step': 1, 'langgraph_node': 'a', 'langgraph_triggers': ['start:a'], ...})\n    (AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' France', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    (AIMessageChunk(content=' Paris', additional_kwargs={}, response_metadata={}, id='...'), {...})\n    \n[/code]\n\n###  `` `invoke(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: StreamMode = 'values', output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, **kwargs: Any) -> Union[dict[str, Any], Any]` \u00b6\n\nRun the graph with a single input and config.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input data for the graph. It can be a dictionary or any other type.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nOptional. The configuration for the graph run.\n\n  * **`stream_mode`** (`StreamMode`, default: `'values'` ) \u2013 \n\nOptional[str]. The stream mode for the graph run. Default is \"values\".\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The output keys to retrieve from the graph run.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt the graph run before.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt the graph run after.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nOptional. Enable debug mode for the graph run.\n\n  * **`**kwargs`** (`Any`, default: `{}` ) \u2013 \n\nAdditional keyword arguments to pass to the graph run.\n\n\n\n\nReturns:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe output of the graph run. If stream_mode is \"values\", it returns the latest output.\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nIf stream_mode is not \"values\", it returns a list of output chunks.\n\n\n\n\n###  `` `ainvoke(input: Union[dict[str, Any], Any], config: Optional[RunnableConfig] = None, *, stream_mode: StreamMode = 'values', output_keys: Optional[Union[str, Sequence[str]]] = None, interrupt_before: Optional[Union[All, Sequence[str]]] = None, interrupt_after: Optional[Union[All, Sequence[str]]] = None, debug: Optional[bool] = None, **kwargs: Any) -> Union[dict[str, Any], Any]` `async` \u00b6\n\nAsynchronously invoke the graph on a single input.\n\nParameters:\n\n  * **`input`** (`Union[dict[str, Any], Any]`) \u2013 \n\nThe input data for the computation. It can be a dictionary or any other type.\n\n  * **`config`** (`Optional[RunnableConfig]`, default: `None` ) \u2013 \n\nOptional. The configuration for the computation.\n\n  * **`stream_mode`** (`StreamMode`, default: `'values'` ) \u2013 \n\nOptional. The stream mode for the computation. Default is \"values\".\n\n  * **`output_keys`** (`Optional[Union[str, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The output keys to include in the result. Default is None.\n\n  * **`interrupt_before`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt before. Default is None.\n\n  * **`interrupt_after`** (`Optional[Union[All, Sequence[str]]]`, default: `None` ) \u2013 \n\nOptional. The nodes to interrupt after. Default is None.\n\n  * **`debug`** (`Optional[bool]`, default: `None` ) \u2013 \n\nOptional. Whether to enable debug mode. Default is None.\n\n  * **`**kwargs`** (`Any`, default: `{}` ) \u2013 \n\nAdditional keyword arguments.\n\n\n\n\nReturns:\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nThe result of the computation. If stream_mode is \"values\", it returns the latest value.\n\n  * `Union[dict[str, Any], Any]` \u2013 \n\nIf stream_mode is \"chunks\", it returns a list of chunks.\n\n\n\n\n##  `` `add_messages(left: Messages, right: Messages, *, format: Optional[Literal['langchain-openai']] = None) -> Messages` \u00b6\n\nMerges two lists of messages, updating existing messages by ID.\n\nBy default, this ensures the state is \"append-only\", unless the new message has the same ID as an existing message.\n\nParameters:\n\n  * **`left`** (`Messages`) \u2013 \n\nThe base list of messages.\n\n  * **`right`** (`Messages`) \u2013 \n\nThe list of messages (or single message) to merge into the base list.\n\n  * **`format`** (`Optional[Literal['langchain-openai']]`, default: `None` ) \u2013 \n\nThe format to return messages in. If None then messages will be returned as is. If 'langchain-openai' then messages will be returned as BaseMessage objects with their contents formatted to match OpenAI message format, meaning contents can be string, 'text' blocks, or 'image_url' blocks and tool responses are returned as their own ToolMessages.\n\n**REQUIREMENT** : Must have `langchain-core>=0.3.11` installed to use this feature.\n\n\n\n\nReturns:\n\n  * `Messages` \u2013 \n\nA new list of messages with the messages from `right` merged into `left`.\n\n  * `Messages` \u2013 \n\nIf a message in `right` has the same ID as a message in `left`, the\n\n  * `Messages` \u2013 \n\nmessage from `right` will replace the message from `left`.\n\n\n\n\nExamples:\n[code] \n    >>> from langchain_core.messages import AIMessage, HumanMessage\n    >>> msgs1 = [HumanMessage(content=\"Hello\", id=\"1\")]\n    >>> msgs2 = [AIMessage(content=\"Hi there!\", id=\"2\")]\n    >>> add_messages(msgs1, msgs2)\n    [HumanMessage(content='Hello', id='1'), AIMessage(content='Hi there!', id='2')]\n    \n    >>> msgs1 = [HumanMessage(content=\"Hello\", id=\"1\")]\n    >>> msgs2 = [HumanMessage(content=\"Hello again\", id=\"1\")]\n    >>> add_messages(msgs1, msgs2)\n    [HumanMessage(content='Hello again', id='1')]\n    \n    >>> from typing import Annotated\n    >>> from typing_extensions import TypedDict\n    >>> from langgraph.graph import StateGraph\n    >>>\n    >>> class State(TypedDict):\n    ...     messages: Annotated[list, add_messages]\n    ...\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"chatbot\", lambda state: {\"messages\": [(\"assistant\", \"Hello\")]})\n    >>> builder.set_entry_point(\"chatbot\")\n    >>> builder.set_finish_point(\"chatbot\")\n    >>> graph = builder.compile()\n    >>> graph.invoke({})\n    {'messages': [AIMessage(content='Hello', id=...)]}\n    \n    >>> from typing import Annotated\n    >>> from typing_extensions import TypedDict\n    >>> from langgraph.graph import StateGraph, add_messages\n    >>>\n    >>> class State(TypedDict):\n    ...     messages: Annotated[list, add_messages(format='langchain-openai')]\n    ...\n    >>> def chatbot_node(state: State) -> list:\n    ...     return {\"messages\": [\n    ...         {\n    ...             \"role\": \"user\",\n    ...             \"content\": [\n    ...                 {\n    ...                     \"type\": \"text\",\n    ...                     \"text\": \"Here's an image:\",\n    ...                     \"cache_control\": {\"type\": \"ephemeral\"},\n    ...                 },\n    ...                 {\n    ...                     \"type\": \"image\",\n    ...                     \"source\": {\n    ...                         \"type\": \"base64\",\n    ...                         \"media_type\": \"image/jpeg\",\n    ...                         \"data\": \"1234\",\n    ...                     },\n    ...                 },\n    ...             ]\n    ...         },\n    ...     ]}\n    >>> builder = StateGraph(State)\n    >>> builder.add_node(\"chatbot\", chatbot_node)\n    >>> builder.set_entry_point(\"chatbot\")\n    >>> builder.set_finish_point(\"chatbot\")\n    >>> graph = builder.compile()\n    >>> graph.invoke({\"messages\": []})\n    {\n        'messages': [\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"Here's an image:\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": \"data:image/jpeg;base64,1234\"},\n                    },\n                ],\n            ),\n        ]\n    }\n    \n[/code]\n\n..versionchanged:: 0.2.61\n[code] \n    Support for 'format=\"langchain-openai\"' flag added.\n    \n[/code]\n\n## Comments\n",
    "metadata": {
        "url": "https://langchain-ai.github.io/langgraph/reference/graphs/",
        "title": "Graphs",
        "description": "Build language agents as graphs",
        "keywords": "No keywords"
    }
}